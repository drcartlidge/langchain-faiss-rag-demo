# -*- coding: utf-8 -*-
"""langchain_RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YtYmDpnV78i2nLsOei7tQaTsZZJ79LxI
"""

!pip install langchain langchain-openai langchain-community faiss-cpu tiktoken

from langchain_openai import OpenAI
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.vectorstores import VectorStoreRetriever
from langchain.chains import RetrievalQA

import os
os.environ["OPENAI_API_KEY"] = ""

loader = TextLoader("mcgraw_hill_wikipedia.txt")

documents = loader.load()

text_spliter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=0,
    length_function=len,
)

docs = text_spliter.split_documents(documents)

embedding = OpenAIEmbeddings() # vector embeddings to be sent to vector library

library = FAISS.from_documents(docs, embedding)

Query1 = 'Who is the president in 2019'

Query_Answer = library.similarity_search(Query1)

print(Query_Answer[0].page_content)

docs_and_scores = library.similarity_search_with_score(Query1)  # scores closer to 0 are better

docs_and_scores[0]

retriever = library.as_retriever()

qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type="stuff", retriever=retriever)

retriever_query = 'Who is the president in 2019'

results = qa.invoke(retriever_query)

print(results)

library.save_local('faiss_index_community_feedback')

faiss_community_feedback_saved = FAISS.load_local('faiss_index_community_feedback', embedding, allow_dangerous_deserialization=True)

qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type="stuff", retriever=retriever)

results = qa.invoke(retriever_query)

print(results)